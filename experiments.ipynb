{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Raptor\n",
    "#### Implementing Raptor index to create summaries at different level.At each level we will prompt Q/A for each summary.\n",
    "\n",
    "1) Ingesting data\n",
    "2) Implementing Raptor\n",
    "3) Query prompt\n",
    "****\n",
    "4) Evaluation criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to be imported\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import umap.umap_ as umap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import umap\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet langchain-core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragatouille 0.0.8.post2 requires langchain<0.2.0,>=0.1.0, but you have langchain 0.2.12 which is incompatible.\n",
      "ragatouille 0.0.8.post2 requires sentence-transformers<3.0.0,>=2.2.2, but you have sentence-transformers 3.0.1 which is incompatible.\n",
      "langchain 0.2.12 requires langchain-core<0.3.0,>=0.2.27, but you have langchain-core 0.1.52 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.1.52 which is incompatible.\n",
      "langchain-groq 0.1.9 requires langchain-core<0.3.0,>=0.2.26, but you have langchain-core 0.1.52 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet  umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragatouille 0.0.8.post2 requires langchain<0.2.0,>=0.1.0, but you have langchain 0.2.12 which is incompatible.\n",
      "ragatouille 0.0.8.post2 requires langchain_core<0.2.0,>=0.1.4, but you have langchain-core 0.2.29 which is incompatible.\n",
      "ragatouille 0.0.8.post2 requires sentence-transformers<3.0.0,>=2.2.2, but you have sentence-transformers 3.0.1 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.29 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet langchain_groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingesting data\n",
    "\n",
    "docs=pd.read_csv(\"C:\\All_projects\\PATH_Opensource\\pop of 6 crops.docx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=docs[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY=\"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\"\n",
    "\n",
    "model_llm= ChatGroq(groq_api_key=GROQ_API_KEY,\n",
    "              model_name='llama3-70b-8192')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raptor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global clutser\n",
    "\n",
    "RANDOM_SEED=42\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "        embeddings:np.ndarray,\n",
    "        dim: int,\n",
    "        n_neighbors: Optional[int] = None,\n",
    "        metric:str = \"cosine\",\n",
    "\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    \"\"\"perform global dimensionality reduction on the embeddings using UMAP.\n",
    "    \n",
    "    parameters:\n",
    "    - embedding: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP \n",
    "    \n",
    "    Returns: A numpy array of the embeddings reduced to the specified dimensionality\"\"\"\n",
    "\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors= int((len(embeddings)-1)** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric \n",
    "    ).fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local clustering\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "        embeddings: np.ndarray, dim:int, num_neighbors:int=10, metric:str=\"cosine\"\n",
    ")-> np.ndarray:\n",
    "    \"\"\"\"Docs  \"\"\"\n",
    "\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim,metric=metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get optimal number of of clusters\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "        embeddings: np.ndarray, max_clusters: int = 50, random_state: int= RANDOM_SEED\n",
    ") -> int:\n",
    "    \n",
    "    max_clusters=min(max_clusters,len(embeddings))\n",
    "    n_clusters= np.arange(1, max_clusters)\n",
    "    bics=[]\n",
    "    \n",
    "    for n in n_clusters:\n",
    "        gm= GaussianMixture(n_components=n,random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to perform cluster embeddibg using a GMM\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state:int=0):\n",
    "\n",
    "    \"\"\"DOCS\"\"\"\n",
    "\n",
    "    n_clusters= get_optimal_clusters(embeddings)\n",
    "    gm= GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob>threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "def perform_clustering(embeddings:np.ndarray, dim: int, threshold: float)-> List[np.ndarray]:\n",
    "    \"\"\"DOCS\"\"\"\n",
    "\n",
    "    if len(embeddings) <= dim+1:\n",
    "        #avoid clustering when theres insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "    # Global dimensionality reduction\n",
    "\n",
    "    reduced_embeddings_global= global_cluster_embeddings(embeddings, dim)\n",
    "\n",
    "    #Global clustering\n",
    "\n",
    "    global_clusters, n_global_clusters =GMM_cluster(\n",
    "        reduced_embeddings_global,threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    #Iterate through each global cluster to perform local clustering\n",
    "\n",
    "    for i in range(n_global_clusters):\n",
    "        #Extract embeddings belonging to the current global cluster\n",
    "\n",
    "        global_cluster_embeddings_= embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_)<= dim +1:\n",
    "            #handle small clusters with direct assignmemt\n",
    "            local_clusters= [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters=1\n",
    "        else:\n",
    "            #Local dimensiioanlity reduction and clustering\n",
    "            reduced_embeddings_local= local_cluster_embeddings(\n",
    "                global_cluster_embeddings_,dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local,threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding text docs\n",
    "\n",
    "def embed_text(docs):\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(docs)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def  cluster_embed_text(texts):\n",
    "    \"\"\"DOCS\"\"\"\n",
    "\n",
    "    text_embedding= embed_text(texts)\n",
    "    cluster_labels= perform_clustering(\n",
    "        text_embedding,10,0.1\n",
    "    )\n",
    "    df_store=pd.DataFrame() # to store the results\n",
    "    df_store[\"text\"]=texts\n",
    "    \n",
    "    \n",
    "    df_store[\"embd\"]= list(text_embedding)\n",
    "    df_store[\"cluster\"]= cluster_labels \n",
    "    return df_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize(\n",
    "        texts:List[str], level:int\n",
    ")-> Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"DOCS\"\"\"\n",
    "\n",
    "    df_clusters = cluster_embed_text(texts)\n",
    "\n",
    "    expanded_list= []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "\n",
    "        # Summarization\n",
    "    template = \"\"\"Here is a document.\n",
    "\n",
    "    Give a detailed summary of the documentation provided.Strictly avoid starting with any such phrases in the output \"Here is a detailed summary of the document provided:\"or \"The documentation appears to be a collection \".Just provide the detailed summary and no other statements.\n",
    "\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model_llm | StrOutputParser()     #where is model and stroutputparser?\n",
    "    \n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "# Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"summaries\": summaries,\n",
    "                \"level\": [level] * len(summaries),\n",
    "                \"cluster\": list(all_clusters),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return df_clusters, df_summary\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3  #form 3 to 4\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    \n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "        \n",
    "    if level == n_levels or unique_clusters == 1:\n",
    "        \n",
    "\n",
    "        f_summary= df_summary[\"summaries\"].tolist()\n",
    "        f_embed=embed_text(f_summary)\n",
    "        \n",
    "        df_final_embeddings = pd.DataFrame(\n",
    "            {\n",
    "                \"text\": f_summary,\n",
    "                \"embd\": list(f_embed),\n",
    "                \"cluster\": df_summary[\"cluster\"],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Append the final summaries embeddings to df_clusters\n",
    "        df_clusters = pd.concat([df_clusters, df_final_embeddings])\n",
    "        \n",
    "        \n",
    "        results[level]=(df_clusters,df_summary)\n",
    "        \n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 9 clusters--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\All_projects\\PATH_Opensource\\mypro\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\All_projects\\PATH_Opensource\\mypro\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "leaf_texts = sample_text\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The document appears to be a guide for farmers...\n",
       "1    The document describes various pests and disea...\n",
       "2    The document provides guidelines for managing ...\n",
       "3    Mangu pod cultivation has a risk of bacterial ...\n",
       "4    Maize is a major crop in the state after paddy...\n",
       "5    The documentation provides guidelines for the ...\n",
       "6    The document appears to be a collection of tab...\n",
       "7    The document appears to be a collection of agr...\n",
       "8    The document describes various types of insect...\n",
       "Name: summaries, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1][1][\"summaries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Q/A queries prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the prompt template\n",
    "\n",
    "template=\"\"\"\n",
    "    You are an AI assistant. Given the following summary, generate a question that could be asked about it, \n",
    "    followed by a detailed answer.Don't output anything like \"Here is a question and answer based on the summary\",just provide the question and the answer in the given format below\n",
    "\n",
    "    Summary: {summary}\n",
    "\n",
    "    Question: \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The document appears to be a comprehensive gui...\n",
       "Name: summaries, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2][1][\"summaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | model_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to store Q&A pairs for each level\n",
    "qa_results = {}\n",
    "\n",
    "# Iterate through each level in the results\n",
    "for level in results.keys():\n",
    "    level_summaries = results[level][1][\"summaries\"]  # Access summaries at the current level\n",
    "    level_qa_pairs = []  # List to store Q&A pairs for the current level\n",
    "\n",
    "    for summary in level_summaries:\n",
    "        qa_pair = chain.invoke({\"summary\": summary})  # Generate Q&A pair for each summary\n",
    "        level_qa_pairs.append(qa_pair)\n",
    "\n",
    "    # Store the Q&A pairs for the current level\n",
    "    qa_results[level] = level_qa_pairs\n",
    "\n",
    "# Now `qa_results` contains the Q&A pairs for each level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['Question: What are some of the specific crops covered in the guide, and what type of information is provided for each crop?\\n\\nAnswer: The guide covers a range of crops, including maize, small grain varieties, pulses, cereals, and hybrid varieties. For each crop, the guide provides information on their characteristics, growing conditions, and specific cultivation practices. For example, for maize, the guide lists various maize varieties, their ripening periods, and yields per hectare, as well as guidance on when to sow and harvest maize. For pulses, the guide mentions that Zahur is the second-largest pulse crop in the state and provides information on sowing and growing pulses, including Jowar, Bajra, and N. Similarly, for cereals, the guide lists various cereal varieties, their characteristics, and growing conditions, including information on species such as wheat, barley, and oats. Overall, the guide provides farmers with valuable information on the specific requirements and cultivation practices for each crop to help them make informed decisions and improve their crop yields.',\n",
       "  'Question: What is the recommended treatment for a Tidal Mite (Caterpillar) infestation in crops?\\n\\nAnswer: The recommended treatment for a Tidal Mite (Caterpillar) infestation in crops is to spray Dimethoate (30 EC) at a rate of 1 liter per hectare or Chlorpyriphos 25 EC at a rate of 1.5-2.0 liters per hectare. This will help control the infestation and prevent further damage to the crops.',\n",
       "  'Question: What are the control measures for Caterpillar pests in pigeon pea and sorghum crops?\\n\\nAnswer: The control measures for Caterpillar pests in pigeon pea and sorghum crops include using Dimethoate 30 EC, Imi Dacloprid 250ml/ha, Indoxacarb 14.5 SC, Kynu Lafas 25 EC, Fenvalerate 20 E.C., and Cypermethrin 10 E.C. These chemicals can be used to control Caterpillar infestations and prevent damage to the crops.',\n",
       "  'Question: How can farmers identify Caterpillar pests in their mangu pod fields?\\n\\nAnswer: Caterpillars can be identified by their yellowish-white color, with a brown or red head that becomes rounded when touched. The larvae, which are about 2 cm long, have a greenish tint and a distinctive perceptual back. Additionally, the damage caused by the caterpillars can be identified by light yellow spots on the pods, and in severe infestations, the pods are crushed, and the grains become small.',\n",
       "  'Question: What are the specific fertilization requirements for maize cultivation, and how can farmers address zinc deficiency in the soil?\\n\\nAnswer: According to the guide, for maize cultivation, farmers should apply fertilizers based on soil test results, and use zinc sulphate to address zinc deficiency in the soil. Additionally, the guide recommends applying 80-100 kg of nitrogen, 40 kg of phosphorus, and 40 kg of potash per hectare, as part of the farm preparation process. By following these fertilizer application guidelines, farmers can ensure that their maize crops receive the necessary nutrients for optimal growth and productivity.',\n",
       "  'Question: What are the treatment options for the Caterpillar pest infestation in Mangu crops?\\n\\nAnswer: The treatment for Caterpillar pest infestation in Mangu crops involves spraying oxydime etone 25% EC at the rate of 1 liter per hectare. This treatment is effective in controlling the hairy caterpillar infestation that typically occurs when the crop is 40-45 days old, which causes leaves to become incapable of preparing food.',\n",
       "  'Question: What is the estimated damage level caused by the Pea Pod Borer to enlarged pods?\\n\\nAnswer: The estimated damage level caused by the Pea Pod Borer to enlarged pods is around 5%. This is because the larvae of this insect eat the pods from the inside, causing damage to the seeds.'],\n",
       " 2: ['Question: What are some of the key pests and diseases that affect crops in Uttar Pradesh, India, and how can farmers manage them according to the guide?\\n\\nAnswer: According to the guide, some of the key pests and diseases that affect crops in Uttar Pradesh, India, include Caterpillar infestations, Termite infestations, Zinc deficiency, Tidal Mite infestations, Yellow Mosaic Disease, Stem Borer Pests, Ear Head Midge, Pea Pod Borer, Bharureng, Hypocritical Leguminer, Newborn Rot, Foliar Rot, Yeflie Yanko, and aphids. To manage these pests and diseases, farmers can adopt integrated pest management practices such as deepening the field in summer to reduce caterpillar population, using seed treatment, and implementing crop management practices to achieve good yields. Additionally, farmers can use specific treatments and prevention methods for each pest and disease, such as identifying and treating Caterpillar infestations, managing irrigation and fertilization practices, and using best practices for cultivating specific crops like maize, sorghum, and millet. By following these guidelines, farmers can reduce crop damage and improve their overall yields.']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
